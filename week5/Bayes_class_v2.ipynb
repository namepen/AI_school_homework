{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "datafile = open('spambase.data', 'r')\n",
    "data = []\n",
    "for line in datafile:\n",
    "    line = [float(element) for element in line.rstrip('\\n').split(',')]\n",
    "    data.append(np.asarray(line))\n",
    "    #rstrip : 지정된 문자열 끝에서 ()내용을 삭제한 새 문자열을 돌려줍니다\n",
    "    #split : 문자열을 나눕니다.\n",
    "    #??데이터를 배열의 형태로 어떻게 변환하였는가? 각 문자를 숫자에 대응??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 48\n",
    "\n",
    "# feature와 label을 구분해줍니다.\n",
    "X = [data[i][:num_features] for i in range(len(data))]\n",
    "y = [int(data[i][-1]) for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train, test 데이터셋으로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, \n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes_Classifier(object):\n",
    "    # 48개의 feature를 이용합니다\n",
    "    def __init__(self, num_features=48):\n",
    "        self.num_features = num_features\n",
    "\n",
    "    # (2) log(P(feature_vector | Class))를 계산합니다.\n",
    "    def log_likelihood_naivebayes(self, feature_vector, Class):\n",
    "        assert len(feature_vector) == self.num_features\n",
    "        log_likelihood = 0.0 #log-likelihood를 사용해 underflow 회피\n",
    "        \n",
    "        if Class == 0:\n",
    "            for i in range(len(feature_vector)):\n",
    "                if(feature_vector[i] == 1):\n",
    "                    log_likelihood += np.log10(self.likelihoods_ham[i])\n",
    "                elif(feature_vector[i] == 0):\n",
    "                    log_likelihood += np.log10(1-self.likelihoods_ham[i])\n",
    "                        \n",
    "        elif Class == 1:\n",
    "             for i in range(len(feature_vector)):\n",
    "                if(feature_vector[i] ==1):\n",
    "                    log_likelihood += np.log10(self.likelihoods_spam[i])\n",
    "                elif(feature_vector[i]==0):\n",
    "                    log_likelihood += np.log10(1- self.likelihoods_spam[i])\n",
    "        else:\n",
    "            raise ValueError(\"Class takes integer values 0 or 1\")\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    # 3. 각 클래스의 Posterior probability를 구합니다.\n",
    "    def class_posteriors(self, feature_vector):\n",
    "        log_likelihood_ham = self.log_likelihood_naivebayes(feature_vector, Class = 0)\n",
    "        log_likelihood_spam = self.log_likelihood_naivebayes(feature_vector, Class = 1)\n",
    "        \n",
    "        log_posterior_ham = log_likelihood_ham + self.log_prior_ham\n",
    "        log_posterior_spam = log_likelihood_spam + self.log_prior_spam\n",
    "        \n",
    "        return log_posterior_ham, log_posterior_spam\n",
    "    \n",
    "    # Maximum A Priori(MAP) inference를 이용해 사후확률이 가장 큰 클래스를 선택합니다.\n",
    "    def spam_classify(self, document):\n",
    "        feature_vector = [int(element>0.0) for element in document]\n",
    "        log_posterior_ham, log_posterior_spam = self.class_posteriors(feature_vector)\n",
    "        \n",
    "        if log_posterior_ham > log_posterior_spam:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "    # (1)모델을 학습하는 train 함수를 작성하세요.\n",
    "    def train(self, X_train, y_train):\n",
    "        # Likelihood estimator 만들기\n",
    "        # 스팸 클래스와 햄 클래스 나누기\n",
    "        X_train_spam = [X_train[i] for i in range(len(X_train)) if y_train[i] == 1]\n",
    "        X_train_ham = [X_train[i] for i in range(len(X_train)) if y_train[i] == 0]\n",
    "    \n",
    "        # 각 클래스의 feature 각각에 대한 likelihood 구하세요. P(X | C)\n",
    "        self.likelihoods_ham = np.mean(X_train_ham, axis=0) / 100\n",
    "        self.likelihoods_spam = np.mean(X_train_spam, axis=0) /100\n",
    "\n",
    "        # 각 class의 log-prior를 계산하세요 -> P(C)\n",
    "        num_ham = float(len(X_train_ham))\n",
    "        num_spam = float(len(X_train_spam))\n",
    "\n",
    "        prior_probability_ham = num_ham / (num_ham + num_spam) #P(C=0)\n",
    "        prior_probability_spam = num_spam / (num_ham + num_spam) # P(C=1)\n",
    "\n",
    "        self.log_prior_ham = np.log10(prior_probability_ham)\n",
    "        self.log_prior_spam = np.log10(prior_probability_spam)\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for case in X_test:\n",
    "            predictions.append(self.spam_classify(case))\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB = Naive_Bayes_Classifier()\n",
    "NB.train(X_train, y_train)\n",
    "pred = NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4126846220677672\n"
     ]
    }
   ],
   "source": [
    "def evaluate_performance(predictions, ground_truth_labels):\n",
    "    correct_count = 0.0\n",
    "    for item_index in range(len(predictions)):\n",
    "        if predictions[item_index] == ground_truth_labels[item_index]:\n",
    "            correct_count += 1.0\n",
    "    accuracy = correct_count/len(predictions)\n",
    "    return accuracy\n",
    "\n",
    "accuracy_naivebayes = evaluate_performance(pred, y_test)\n",
    "print(accuracy_naivebayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
